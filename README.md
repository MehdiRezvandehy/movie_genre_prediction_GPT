# Movie Genre Prediction by Fine-tuning GPT

GPT undergoes pre-training by exposing the model to a substantial text corpus called BookCorpus, comprising 4.5 GB of text sourced from 7,000 unpublished books spanning various genres. In this pre-training phase, the model learns to predict the next word in a sequence based on preceding words, a process known as language modeling. This helps impart an understanding of the inherent structure and patterns of natural language. Following pre-training, GPT can be fine-tuned for specific tasks using a more focused, task-specific dataset. Fine-tuning involves adjusting the model's parameters to enhance its suitability for the given task, such as classification, similarity scoring, or question answering. Over time, OpenAI has enhanced and expanded the GPT architecture, introducing subsequent models like GPT-2, GPT-3, GPT-3.5, and GPT-4. These newer models are trained on larger datasets and boast increased capacities, enabling them to generate more intricate and coherent text. The widespread adoption of GPT models by both researchers and industry practitioners has played a pivotal role in advancing natural language processing tasks significantly. Only GPT-2 is freely available on [HuggingFace](https://huggingface.co/gpt2). In this notebook, [distilgpt2](https://huggingface.co/distilgpt2) model (more efficient version of the GPT-2) was fine-tuned to predict movie genre prediction.
